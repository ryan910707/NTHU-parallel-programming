<?xml version="1.0" encoding="iso-8859-1"?>
<ipm_job_profile>
<calltable nsections="1" >
<section module="MPI" nentries="69" >
<entry name="MPI_Init" />
<entry name="MPI_Init_thread" />
<entry name="MPI_Finalize" />
<entry name="MPI_Comm_rank" />
<entry name="MPI_Comm_size" />
<entry name="MPI_Send" />
<entry name="MPI_Ssend" />
<entry name="MPI_Rsend" />
<entry name="MPI_Bsend" />
<entry name="MPI_Isend" />
<entry name="MPI_Issend" />
<entry name="MPI_Irsend" />
<entry name="MPI_Ibsend" />
<entry name="MPI_Recv" />
<entry name="MPI_Irecv" />
<entry name="MPI_Sendrecv" />
<entry name="MPI_Sendrecv_replace" />
<entry name="MPI_Wait" />
<entry name="MPI_Waitany" />
<entry name="MPI_Waitall" />
<entry name="MPI_Waitsome" />
<entry name="MPI_Probe" />
<entry name="MPI_Iprobe" />
<entry name="MPI_Send_init" />
<entry name="MPI_Ssend_init" />
<entry name="MPI_Rsend_init" />
<entry name="MPI_Bsend_init" />
<entry name="MPI_Recv_init" />
<entry name="MPI_Buffer_attach" />
<entry name="MPI_Buffer_detach" />
<entry name="MPI_Test" />
<entry name="MPI_Testany" />
<entry name="MPI_Testall" />
<entry name="MPI_Testsome" />
<entry name="MPI_Start" />
<entry name="MPI_Startall" />
<entry name="MPI_Bcast" />
<entry name="MPI_Reduce" />
<entry name="MPI_Reduce_scatter" />
<entry name="MPI_Barrier" />
<entry name="MPI_Gather" />
<entry name="MPI_Gatherv" />
<entry name="MPI_Scatter" />
<entry name="MPI_Scatterv" />
<entry name="MPI_Scan" />
<entry name="MPI_Allgather" />
<entry name="MPI_Allgatherv" />
<entry name="MPI_Allreduce" />
<entry name="MPI_Alltoall" />
<entry name="MPI_Alltoallv" />
<entry name="MPI_Comm_group" />
<entry name="MPI_Comm_compare" />
<entry name="MPI_Comm_dup" />
<entry name="MPI_Comm_create" />
<entry name="MPI_Comm_split" />
<entry name="MPI_Comm_free" />
<entry name="MPI_Ibcast" />
<entry name="MPI_Ireduce" />
<entry name="MPI_Ireduce_scatter" />
<entry name="MPI_Igather" />
<entry name="MPI_Igatherv" />
<entry name="MPI_Iscatter" />
<entry name="MPI_Iscatterv" />
<entry name="MPI_Iscan" />
<entry name="MPI_Iallgather" />
<entry name="MPI_Iallgatherv" />
<entry name="MPI_Iallreduce" />
<entry name="MPI_Ialltoall" />
<entry name="MPI_Ialltoallv" />
</section>
</calltable>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="0" mpi_size="1" stamp_init="1699687136.585551" stamp_final="1699687238.133950" username="pp23s09" allocationname="unknown" flags="0" pid="1702068" >
<job nhosts="1" ntasks="1" start="1699687136" final="1699687238" cookie="nocookie" code="unknown" >4924409</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >apollo35</host>
<perf wtime="1.01548e+02" utime="1.00930e+02" stime="1.09834e-01" mtime="1.08361e-02" gflop="0.00000e+00" gbyte="1.28937e-01" omp_num_threads="1"></perf>
<modules nmod="2">
<module name="MPI" time="1.08361e-02" ></module>
<module name="PAPI" time="0.0" ncpu="6" nnodes="2" totalcpus="12" threads="1" cores="6"  vendor="1" vendor_string="GenuineIntel" model="44" model_string="Intel(R) Xeon(R) CPU           X5670  @ 2.93GHz" revision="2.000000" min_mhz="1600" max_mhz="2934" domain="15"></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/home/pp23/pp23s09/PP/hw2/hw2b" md5sum="91b19ec9db7f807fb955b9550000807f59" >./hw2b ./exp.png 174170376 -0.7894722222222 -0.7825277777777 0.1450468 0.1489531 2549 1439 </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.01424e+02" utime="1.00930e+02" stime="1.03228e-01" mtime="1.08361e-02" id="0">
<modules nmod="2">
<module name="MPI" time="1.08361e-02" ></module>
<module name="PAPI" time="0.0" ncpu="6" nnodes="2" totalcpus="12" threads="1" cores="6"  vendor="1" vendor_string="GenuineIntel" model="44" model_string="Intel(R) Xeon(R) CPU           X5670  @ 2.93GHz" revision="2.000000" min_mhz="1600" max_mhz="2934" domain="15"></module>
</modules>
<hpm api="PAPI" ncounter="0" eventset="0" gflop="0.00000e+00" >
</hpm>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="1" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Gather" count="1" bytes="1.2583e+07" > 1.0833e-02 </func>
</region>
</regions>
<hash nlog="5" nkey="5" >
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" callsite="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" callsite="0" count="1" tid="0" op="" dtype="" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" callsite="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" callsite="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A0001000000040000C0000000000000" call="MPI_Gather" bytes="12582912" orank="0" region="0" callsite="0" count="1" tid="0" op="" dtype="MPI_INT" >1.0833e-02 1.0833e-02 1.0833e-02</hent>
</hash>
<internal rank="0" log_i="1699687238.133950" log_t="1.6997e+09" report_delta="-1.0000e+00" fname="./pp23s09.1699687136.585551.ipm.xml" logrank="0" ></internal>
</task>
</ipm_job_profile>
